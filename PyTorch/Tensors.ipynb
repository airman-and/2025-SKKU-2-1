{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 파이토치와 넘파이 모듈을 import하는 과정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49b2e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "# 파이토치의 tensor를 생성하는 과정정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52413ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "\n",
    "# 넘파이로 생성한 배열을 파이토치의 tensor로 변환하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4094b422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      "tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      "tensor([[0.9660, 0.8595],\n",
      "        [0.6776, 0.6279]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data)  # x_data와 같은 크기의 텐서를 생성\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float)  # x_data와 같은 크기의 랜덤 텐서를 생성\\\n",
    "print(f\"Ones Tensor: \\n{x_ones} \\n\")\n",
    "print(f\"Random Tensor: \\n{x_rand} \\n\")\n",
    "# 네, 맞아요! x_data와 동일한 shape을 가지면서, 값은 모두 1이거나(ones_like), 0~1 사이의 랜덤값(rand_like)으로 채워진 텐서를 생성하는 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f57056bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor with shape (2, 3): \n",
      "tensor([[0.0148, 0.2284, 0.8465],\n",
      "        [0.3761, 0.7221, 0.7440]]) \n",
      "\n",
      "Ones Tensor with shape (2, 3): \n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor with shape (2, 3): \n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]]) \n",
      "\n",
      "tensor([[[0.2602, 0.4719, 0.4669, 0.1427, 0.6810],\n",
      "         [0.6590, 0.4341, 0.4691, 0.5906, 0.5982],\n",
      "         [0.5334, 0.9127, 0.4013, 0.3471, 0.1583],\n",
      "         [0.4974, 0.4157, 0.6006, 0.3386, 0.1991],\n",
      "         [0.9607, 0.8800, 0.9755, 0.4046, 0.7035]],\n",
      "\n",
      "        [[0.4164, 0.7476, 0.7618, 0.8544, 0.0223],\n",
      "         [0.1989, 0.9639, 0.2227, 0.5991, 0.6823],\n",
      "         [0.2175, 0.8278, 0.0040, 0.3746, 0.8585],\n",
      "         [0.6427, 0.7277, 0.0332, 0.9841, 0.0525],\n",
      "         [0.2891, 0.0182, 0.8227, 0.1017, 0.2432]],\n",
      "\n",
      "        [[0.6066, 0.8384, 0.1012, 0.2293, 0.1283],\n",
      "         [0.9340, 0.0678, 0.9944, 0.1375, 0.3401],\n",
      "         [0.9685, 0.8032, 0.3260, 0.9006, 0.3390],\n",
      "         [0.8246, 0.9884, 0.2174, 0.3131, 0.2595],\n",
      "         [0.2129, 0.2018, 0.2075, 0.0189, 0.6916]],\n",
      "\n",
      "        [[0.3107, 0.6480, 0.3636, 0.7668, 0.5578],\n",
      "         [0.1133, 0.8140, 0.3395, 0.1595, 0.8996],\n",
      "         [0.5743, 0.8869, 0.8320, 0.0616, 0.5297],\n",
      "         [0.8432, 0.3179, 0.3843, 0.8545, 0.7216],\n",
      "         [0.5928, 0.6572, 0.1732, 0.1145, 0.8887]],\n",
      "\n",
      "        [[0.3367, 0.1550, 0.9608, 0.7924, 0.9419],\n",
      "         [0.0609, 1.0000, 0.3212, 0.2616, 0.1245],\n",
      "         [0.9447, 0.7377, 0.1303, 0.4820, 0.2794],\n",
      "         [0.9843, 0.5633, 0.8565, 0.9522, 0.1924],\n",
      "         [0.1153, 0.1435, 0.2238, 0.9563, 0.5591]]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "print(f\"Random Tensor with shape {shape}: \\n{rand_tensor} \\n\")\n",
    "# 지정한 shape을 가지는 랜덤 텐서를 생성하는 과정\n",
    "ones_tensor = torch.ones(shape)\n",
    "print(f\"Ones Tensor with shape {shape}: \\n{ones_tensor} \\n\")\n",
    "# 지정한 shape을 가지는 값이 모두 1인 텐서를 생성하는 과정\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "print(f\"Zeros Tensor with shape {shape}: \\n{zeros_tensor} \\n\") \n",
    "rand_ten = torch.rand(5,5,5)\n",
    "print(rand_ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f225b1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor =  torch.rand(3,4)\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0a314de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device tensor is stored on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")\n",
    "    print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce3a9f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "tensor[:, 1] = 0  # 두 번째 열을 0으로 설정\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64767e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)\n",
    "# tensor를 세로로 이어붙이는 과정\n",
    "t2 = torch.cat([tensor, tensor, tensor], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d790f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor): \n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor * tensor: \n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor.matmul(tensor.T): \n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) \n",
      "\n",
      "tensor @ tensor.T: \n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]]) \n",
      "\n",
      "tensor.sum(): \n",
      "12.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"tensor.mul(tensor): \\n{tensor.mul(tensor)} \\n\")\n",
    "print(f\"tensor * tensor: \\n{tensor * tensor} \\n\")\n",
    "print(f\"tensor.matmul(tensor.T): \\n{tensor.matmul(tensor.T)} \\n\")\n",
    "print(f\"tensor @ tensor.T: \\n{tensor @ tensor.T} \\n\")\n",
    "print(f\"tensor.sum(): \\n{tensor.sum()} \\n\")\n",
    "# mul은 두 텐서의 원소별(element-wise) 곱셈을 의미합니다.\n",
    "# 즉, tensor.mul(tensor)는 같은 위치의 값끼리 곱해서 새로운 텐서를 만듭니다.\n",
    "# tensor * tensor 역시 원소별 곱셈을 의미하며, mul과 동일한 결과를 냅니다.\n",
    "# tensor.matmul(tensor.T)는 텐서와 그 전치행렬(T) 간의 행렬 곱을 수행합니다.\n",
    "# tensor @ tensor.T 역시 행렬 곱 연산자로, matmul과 같은 결과를 반환합니다.\n",
    "# tensor.sum()은 텐서의 모든 원소의 합을 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea467e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[21., 20., 21., 21.],\n",
      "        [21., 20., 21., 21.],\n",
      "        [21., 20., 21., 21.],\n",
      "        [21., 20., 21., 21.]])\n"
     ]
    }
   ],
   "source": [
    "tensor.add_(5)\n",
    "print(tensor)\n",
    "# 인플레이스 연산은 메모리를 절약할 수 있지만, 미분을 계산할 때 \n",
    "# 연산 이력이 즉시 사라지기 때문에 문제가 될 수 있습니다. \n",
    "# 따라서 인플레이스 연산의 사용은 권장되지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8c741c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor to numpy: \n",
      "[[21. 20. 21. 21.]\n",
      " [21. 20. 21. 21.]\n",
      " [21. 20. 21. 21.]\n",
      " [21. 20. 21. 21.]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = tensor.numpy()\n",
    "print(f\"tensor to numpy: \\n{n} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c206581",
   "metadata": {},
   "outputs": [],
   "source": [
    "`out` 파라미터는 결과를 저장할 배열을 지정하는 역할을 합니다.  \n",
    "즉, `np.add(n, 1, out=n)`은 `n + 1`의 결과를 새로운 배열에 저장하지 않고, 기존의 `n` 배열에 바로 덮어씁니다(메모리 절약, 인플레이스 연산).  \n",
    "따라서 `n`의 값이 직접 변경됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b106224",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
